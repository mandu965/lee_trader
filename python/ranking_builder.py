"""
ranking_builder.py

Build the final per-stock ranking table from:

- data/predictions.csv   (model outputs)
- data/scores_final.csv  (technical composite score)
- data/features.csv      (price/indicators + quality_score)
- data/universe.csv      (code, name, market, sector)
- data/market_status.csv (KOSPI regime info)

í•­ìƒ ì¢…ëª© ë­í‚¹ì€ ë§Œë“¤ê³ ,
market_status.csv ì—ì„œ ì½ì€ ì‹œì¥ ìƒíƒœ(ì‹œì¥ ìƒìŠ¹/í•˜ë½ ë° ì§€í‘œ)ë¥¼
ê° rowì— meta ì»¬ëŸ¼ìœ¼ë¡œ ë¶™ì—¬ì¤€ë‹¤.

ì¶”ê°€ë˜ëŠ” ì»¬ëŸ¼:
- tech_score, pred_score, prob_score, qual_score, final_score
- market_up               (bool)
- market_status_date      (str)
- market_kospi_close      (float)
- market_kospi_ma20       (float)
- market_vol_5d           (float)
- market_foreign_5d       (float)
- generated_at            (str)
"""
import logging
from datetime import datetime
from pathlib import Path

import numpy as np
import pandas as pd
import sqlite3

DATA_DIR = Path("data")

PREDICTIONS_CSV = DATA_DIR / "predictions.csv"
SCORES_CSV = DATA_DIR / "scores_final.csv"
FEATURES_CSV = DATA_DIR / "features.csv"
UNIVERSE_CSV = DATA_DIR / "universe.csv"
MARKET_STATUS_CSV = DATA_DIR / "market_status.csv"

OUT_CSV = DATA_DIR / "ranking_final.csv"
DB_PATH = DATA_DIR / "lee_trader.db"

# -------------------------------
# V2 Scoring Weights
# -------------------------------
# pred_score : ì˜ˆì¸¡ ìˆ˜ìµë¥  ê¸°ë°˜ ì ìˆ˜ (return_score)
# prob_score : ìƒìœ„ 20% ì•ˆì— ë“¤ í™•ë¥  ì ìˆ˜
# tech_score : ê¸°ìˆ ì  íŒ¨í„´ ì ìˆ˜ (ì°¨íŠ¸/ëª¨ë©˜í…€)
# safety_score : ë³€ë™ì„± ë‚®ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
# qual_score : ì¬ë¬´ í€„ë¦¬í‹° ì ìˆ˜
# liquidity_score : ê±°ë˜ëŸ‰(ìœ ë™ì„±) ì ìˆ˜

WEIGHT_TECH = 0.15         # ê¸°ìˆ ì  íŒ¨í„´
WEIGHT_PRED = 0.30         # ì˜ˆì¸¡ ìˆ˜ìµ(í•µì‹¬)
WEIGHT_PROB = 0.25         # ìƒìœ„20% í™•ë¥ 
WEIGHT_SAFETY = 0.15       # ë¦¬ìŠ¤í¬(ë³€ë™ì„±) ë‚®ì„ìˆ˜ë¡ +
WEIGHT_QUAL = 0.10         # ì¬ë¬´ í€„ë¦¬í‹°
WEIGHT_LIQUIDITY = 0.05    # ìœ ë™ì„±

# Risk penalty ì„¤ì •: -15%ê¹Œì§€ëŠ” ê°ì  ì—†ìŒ, ê·¸ ì•„ë˜ë¡œëŠ” ì ì  ê°ì 
RISK_MDD_THRESHOLD = 0.15   # 15% drawdownê¹Œì§€ëŠ” í—ˆìš©
RISK_PENALTY_SCALE = 100.0  # penalty_raw(0~0.3 ì •ë„)ë¥¼ ì ìˆ˜ ìŠ¤ì¼€ì¼ë¡œ ë§ì¶°ì£¼ê¸°


# Risk penalty ì„¤ì •: -15%ê¹Œì§€ëŠ” ê°ì  ì—†ìŒ, ê·¸ ì•„ë˜ë¡œëŠ” ì ì  ê°ì 
RISK_MDD_THRESHOLD = 0.15   # 15% drawdownê¹Œì§€ëŠ” í—ˆìš©
RISK_PENALTY_SCALE = 100.0  # penalty_raw(0~0.3 ì •ë„)ë¥¼ ì ìˆ˜ ìŠ¤ì¼€ì¼ë¡œ ë§ì¶°ì£¼ê¸°

def setup_logging() -> None:
    logging.basicConfig(
        level=logging.INFO,
        format="[%(asctime)s] [%(levelname)s] %(message)s",
    )


def ensure_data_dir() -> None:
    DATA_DIR.mkdir(exist_ok=True, parents=True)


def _load_csv(path: Path, required: bool = True) -> pd.DataFrame:
    if not path.exists():
        if required:
            raise FileNotFoundError(f"Required input CSV not found: {path}")
        logging.warning("Optional input CSV not found: %s", path)
        return pd.DataFrame()
    df = pd.read_csv(path)
    logging.info("Loaded %s (rows=%d)", path, len(df))
    return df


def _clip01(series: pd.Series, lower: float, upper: float) -> pd.Series:
    return series.astype(float).clip(lower=lower, upper=upper)


def _percentile_by_date(df: pd.DataFrame, col: str) -> pd.Series:
    """
    Compute 0~100 percentile (rank) of `col` within each date group.
    Higher values -> higher percentile.
    """
    if col not in df.columns:
        return pd.Series(np.nan, index=df.index)

    def _rank(s: pd.Series) -> pd.Series:
        return s.rank(pct=True, ascending=True) * 100.0

    ranked = df.groupby("date", group_keys=False)[col].transform(_rank)
    return ranked


def _normalize_date(df: pd.DataFrame) -> pd.DataFrame:
    if "date" not in df.columns or df.empty:
        return df
    df = df.copy()
    df["date"] = pd.to_datetime(df["date"]).dt.strftime("%Y-%m-%d")
    return df


def _load_market_status():
    """
    market_status.csv ì—ì„œ ìµœì‹  ì‹œì¥ ìƒíƒœì™€ ì§€í‘œë“¤ì„ ì½ì–´ì˜¨ë‹¤.

    return:
        market_up (bool),
        info (dict) â€“ {
            "date": str,
            "kospi_close": float,
            "kospi_ma20": float,
            "volatility_5d": float,
            "foreign_net_5d": float,
        }
    """
    if not MARKET_STATUS_CSV.exists():
        logging.warning("market_status.csv not found; default market_up=True")
        return True, {}

    try:
        df = pd.read_csv(MARKET_STATUS_CSV)
    except Exception:
        logging.exception("Failed to read market_status.csv; default market_up=True")
        return True, {}

    if df.empty or "market_up" not in df.columns:
        logging.warning("market_status.csv empty or missing market_up; default True")
        return True, {}

    last = df.iloc[-1]

    raw = last["market_up"]
    if isinstance(raw, bool):
        market_up = raw
    else:
        v = str(raw).strip().lower()
        market_up = v in {"true", "1", "t", "y", "yes"}

    info = {}
    for col in ["date", "kospi_close", "kospi_ma20", "volatility_5d", "foreign_net_5d"]:
        if col in last.index:
            info[col] = last[col]

    logging.info(
        "Loaded market status: market_up=%s, info=%s",
        market_up,
        {k: info.get(k) for k in ["date", "kospi_close", "kospi_ma20", "volatility_5d", "foreign_net_5d"]},
    )
    return market_up, info


def build_ranking() -> pd.DataFrame:
    # ---------------------------------------------
    # 1. ì›ë³¸ CSV ë¡œë“œ
    # ---------------------------------------------
    preds = _load_csv(PREDICTIONS_CSV, required=True)
    scores = _load_csv(SCORES_CSV, required=True)
    feats = _load_csv(FEATURES_CSV, required=True)
    universe = _load_csv(UNIVERSE_CSV, required=False)

    # ë‚ ì§œ í¬ë§· í†µì¼
    preds = _normalize_date(preds)
    scores = _normalize_date(scores)
    feats = _normalize_date(feats)

    # ê¸°ë³¸ sanity check
    for df, name in [
        (preds, "predictions"),
        (scores, "scores_final"),
        (feats, "features"),
    ]:
        if df.empty:
            raise RuntimeError(f"{name} is empty â€“ cannot build ranking.")

    # codeë¥¼ ëª¨ë‘ ë¬¸ìì—´/6ìë¦¬ë¡œ í†µì¼
    for df in [preds, scores, feats, universe]:
        if df is not None and not df.empty and "code" in df.columns:
            df["code"] = df["code"].astype(str).str.zfill(6)

    # ---------------------------------------------
    # 2. ë³‘í•© (predictions ê¸°ì¤€)
    # ---------------------------------------------
    base = preds.merge(
        scores,
        on=["date", "code"],
        how="left",
        suffixes=("", "_score"),
    )

    # featuresì—ì„œ í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì‚¬ìš© (close, quality_score, ë³€ë™ì„±/ìœ ë™ì„± ë“±)
    feat_cols = ["date", "code", "close"]

    # ì¬ë¬´ í€„ë¦¬í‹°
    if "quality_score" in feats.columns:
        feat_cols.append("quality_score")

    # ë³€ë™ì„± / ìœ ë™ì„± ê´€ë ¨ í”¼ì²˜ (ìˆì„ ë•Œë§Œ ì‚¬ìš©)
    for col in ["vol_20", "vol_60", "vol_ma_20", "volume"]:
        if col in feats.columns:
            feat_cols.append(col)

    base = base.merge(
        feats[feat_cols],
        on=["date", "code"],
        how="left",
        suffixes=("", "_feat"),
    )


    logging.info(
        "Base merged shape (preds + scores + features): %s",
        base.shape,
    )

    # universeì—ì„œ name, market, sector, etc. ë¶™ì´ê¸° (ì„ íƒ)
    if universe is not None and not universe.empty and "code" in universe.columns:
        base = base.merge(
            universe,
            on="code",
            how="left",
            suffixes=("", "_univ"),
        )
        logging.info("After universe merge shape: %s", base.shape)

    if base.empty:
        raise RuntimeError(
            "No rows after merging predictions/scores/features â€“ cannot build ranking."
        )
    # ---------------------------------------------
    # 3. ì ìˆ˜ ê³„ì‚° (V2)
    # ---------------------------------------------

    # 3-1) tech_score: scores_final.scoreë¥¼ 0~100ìœ¼ë¡œ clip
    # if "score" in base.columns:
    #     base["tech_score"] = _clip01(base["score"].fillna(0.0), 0.0, 100.0)
    # else:
    #     logging.warning("'score' column not found; tech_score will be NaN.")
    #     base["tech_score"] = np.nan

    # 3-1) tech_score: scores_final.csvì—ì„œ ì˜¨ ê¸°ìˆ  ì ìˆ˜ ì‚¬ìš©
    #   - score_score: ê³¼ê±° scoring.pyì—ì„œ ë§Œë“  ê¸°ìˆ  ì ìˆ˜
    #   - composite:   ì¶”ê°€ë¡œ ë§Œë“  ì¢…í•© ê¸°ìˆ  ì ìˆ˜ë¼ë©´ ì´ìª½ì„ ìš°ì„  ì‚¬ìš©í•´ë„ ë¨

    if "composite" in base.columns:
        # compositeì´ ë” ì¢…í•©ì ì¸ ê¸°ìˆ ì ìˆ˜ë¼ë©´ ì´ê±¸ ì“°ì
        base["tech_score"] = _percentile_by_date(base, "composite")
    elif "score_score" in base.columns:
        # ì•„ë‹ˆë©´ score_scoreë¥¼ ë‚ ì§œë³„ percentileë¡œ ë³€í™˜ (0~100)
        base["tech_score"] = _percentile_by_date(base, "score_score")
    else:
        logging.warning(
            "No 'composite' or 'score_score' column found; tech_score will be NaN."
        )
        base["tech_score"] = np.nan
    

    # 3-2) pred_score (return_score):
    #   pred_return_60dì™€ pred_return_90d ë‘˜ ë‹¤ ìˆìœ¼ë©´ 0.6 : 0.4 ê°€ì¤‘ í‰ê· 
    pred_60 = None
    pred_90 = None

    if "pred_return_60d" in base.columns:
        base["pred_score_60"] = _percentile_by_date(base, "pred_return_60d")
        pred_60 = base["pred_score_60"]
    if "pred_return_90d" in base.columns:
        base["pred_score_90"] = _percentile_by_date(base, "pred_return_90d")
        pred_90 = base["pred_score_90"]

    if (pred_60 is not None) and (pred_90 is not None):
        base["pred_score"] = 0.6 * pred_60 + 0.4 * pred_90
    elif pred_60 is not None:
        base["pred_score"] = pred_60
    elif pred_90 is not None:
        base["pred_score"] = pred_90
    else:
        logging.warning(
            "No 'pred_return_60d' or 'pred_return_90d' columns; pred_score will be NaN."
        )
        base["pred_score"] = np.nan

    # ğŸ”¥ Node /api/top20 ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì´ë¦„(ret_score)ì€ pred_scoreì™€ ë™ì¼í•˜ê²Œ ìœ ì§€
    base["ret_score"] = base["pred_score"]

    # 3-3) prob_score: prob_top20_60d * 100  (ë¶„ë¥˜ ëª¨ë¸ í™•ë¥  í™œìš©)
    if "prob_top20_60d" in base.columns:
        base["prob_score"] = _clip01(
            base["prob_top20_60d"].fillna(0.0) * 100.0,
            0.0,
            100.0,
        )
    else:
        logging.warning("'prob_top20_60d' column not found; prob_score will be NaN.")
        base["prob_score"] = np.nan

    # 3-4) qual_score: quality_scoreì˜ ë‚ ì§œë³„ percentile (0~100)
    if "quality_score" in base.columns:
        base["qual_score"] = _percentile_by_date(base, "quality_score")
    else:
        logging.warning("'quality_score' column not found; qual_score will be NaN.")
        base["qual_score"] = np.nan

    # 3-5) safety_score: ë³€ë™ì„±(vol_20, vol_60)ì´ ë‚®ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
    safety_parts = []

    if "vol_20" in base.columns:
        base["vol_20_pct"] = _percentile_by_date(base, "vol_20")
        # ë³€ë™ì„± ë‚®ì„ìˆ˜ë¡ ì¢‹ìœ¼ë¯€ë¡œ 100 - percentile
        safety_parts.append(100.0 - base["vol_20_pct"])

    if "vol_60" in base.columns:
        base["vol_60_pct"] = _percentile_by_date(base, "vol_60")
        safety_parts.append(100.0 - base["vol_60_pct"])

    if safety_parts:
        # ì—¬ëŸ¬ ê°œê°€ ìˆìœ¼ë©´ ë‹¨ìˆœ í‰ê·  (0~100)
        base["safety_score"] = sum(safety_parts) / len(safety_parts)
    else:
        logging.info("No vol_20 / vol_60 columns; safety_score will be NaN.")
        base["safety_score"] = np.nan

    # 3-6) liquidity_score: ìµœê·¼ 20ì¼ í‰ê·  ê±°ë˜ëŸ‰ ê¸°ì¤€ (vol_ma_20 ìš°ì„ )
    if "vol_ma_20" in base.columns:
        base["liquidity_score"] = _percentile_by_date(base, "vol_ma_20")
    elif "volume" in base.columns:
        base["liquidity_score"] = _percentile_by_date(base, "volume")
    else:
        logging.info(
            "No vol_ma_20 / volume columns; liquidity_score will be NaN."
        )
        base["liquidity_score"] = np.nan

    # NaN component scores -> 0 (ì ìˆ˜ ê³„ì‚°ì—ì„œ ê²°ì¸¡ì¹˜ëŠ” 0ì  ì²˜ë¦¬)
    for col in [
        "tech_score",
        "pred_score",
        "prob_score",
        "qual_score",
        "safety_score",
        "liquidity_score",
    ]:
        base[col] = base[col].fillna(0.0)


    # ---------------------------------------------
    # 4. ê¸°ë³¸ ì¢…í•© ì ìˆ˜ (íšŒê·€ + ë¶„ë¥˜ + ê¸°ìˆ  + í€„ë¦¬í‹° + ë¦¬ìŠ¤í¬ + ìœ ë™ì„±)
    # ---------------------------------------------
    base["final_score"] = (
        WEIGHT_TECH * base["tech_score"]
        + WEIGHT_PRED * base["pred_score"]
        + WEIGHT_PROB * base["prob_score"]
        + WEIGHT_QUAL * base["qual_score"]
        + WEIGHT_SAFETY * base["safety_score"]
        + WEIGHT_LIQUIDITY * base["liquidity_score"]
    )


    # ---------------------------------------------
    # 5. ë¦¬ìŠ¤í¬(ì˜ˆì¸¡ MDD) ê¸°ë°˜ ê°ì  ì ìš©
    #    pred_mdd_60dê°€ í´ìˆ˜ë¡(ë‚™í­ì´ ê¹Šì„ìˆ˜ë¡) final_scoreë¥¼ ê¹ìŒ
    # ---------------------------------------------
    if "pred_mdd_60d" in base.columns:
        # pred_mdd_60d: ìŒìˆ˜(ì˜ˆ: -0.25 = -25% ìµœëŒ€ ë‚™í­ ì˜ˆìƒ)
        dd = pd.to_numeric(base["pred_mdd_60d"], errors="coerce")

        # threshold(ì˜ˆ: 0.15 = -15%)ê¹Œì§€ëŠ” ê°ì  ì—†ìŒ,
        # ê·¸ ì•„ë˜ë¶€í„° penalty_raw ì¦ê°€
        #   penalty_raw = max(0, -dd - RISK_MDD_THRESHOLD)
        penalty_raw = (-dd) - RISK_MDD_THRESHOLD
        penalty_raw = penalty_raw.clip(lower=0)  # ìŒìˆ˜ëŠ” 0ìœ¼ë¡œ

        # ìŠ¤ì¼€ì¼(ì˜ˆ: 100 * 0.3 = 30ì  ê°ì  ë“±) ê³±í•´ì„œ ìµœì¢… ê°ì ê°’ ê³„ì‚°
        base["risk_penalty"] = penalty_raw * RISK_PENALTY_SCALE

        # final_scoreì—ì„œ ê°ì  ì ìš©
        base["final_score"] = base["final_score"] - base["risk_penalty"]
    else:
        # pred_mdd_60dê°€ ì—†ìœ¼ë©´ ê°ì  ì—†ì´ 0
        base["risk_penalty"] = 0.0

    # ---------------------------------------------
    # 6. ì •ë ¬ (ìµœì‹  ë‚ ì§œ + ë†’ì€ ì ìˆ˜ ìˆœ)
    # ---------------------------------------------
    base["date"] = pd.to_datetime(base["date"])
    base = base.sort_values(
        ["date", "final_score"],
        ascending=[False, False],
    )
    base["date"] = base["date"].dt.strftime("%Y-%m-%d")

    # ---------------------------------------------
    # 7. ì‹œì¥ ìƒíƒœ ë©”íƒ€ ì •ë³´ ë¶™ì´ê¸°
    # ---------------------------------------------
    market_up, mkt_info = _load_market_status()
    base["market_up"] = market_up
    base["market_status_date"] = mkt_info.get("date")

    # ìˆ˜ì¹˜í˜•ìœ¼ë¡œ ë³€í™˜
    base["market_kospi_close"] = pd.to_numeric(
        mkt_info.get("kospi_close"),
        errors="coerce",
    )
    base["market_kospi_ma20"] = pd.to_numeric(
        mkt_info.get("kospi_ma20"),
        errors="coerce",
    )
    base["market_vol_5d"] = pd.to_numeric(
        mkt_info.get("volatility_5d"),
        errors="coerce",
    )
    base["market_foreign_5d"] = pd.to_numeric(
        mkt_info.get("foreign_net_5d"),
        errors="coerce",
    )

    # ìƒì„± ì‹œê°
    base["generated_at"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    return base



def save_ranking(df: pd.DataFrame) -> None:
    ensure_data_dir()
    df_out = df.copy()
    df_out["date"] = pd.to_datetime(df_out["date"]).dt.strftime("%Y-%m-%d")
    # ensure model_version exists (even as None) for DB binding
    if "model_version" not in df_out.columns:
        df_out["model_version"] = None
    df_out.to_csv(OUT_CSV, index=False, encoding="utf-8")
    logging.info("Saved ranking: %s (rows=%d)", OUT_CSV.resolve(), len(df_out))

    # DB upsert
    conn = None
    try:
        conn = sqlite3.connect(DB_PATH)
        conn.execute("PRAGMA foreign_keys = ON;")
        conn.execute(
            """
            CREATE TABLE IF NOT EXISTS daily_ranking (
                date                 DATE NOT NULL,
                code                 TEXT NOT NULL,
                close                REAL,
                pred_return_60d      REAL,
                pred_return_90d      REAL,
                pred_mdd_60d         REAL,
                pred_mdd_90d         REAL,
                prob_top20_60d       REAL,
                prob_top20_90d       REAL,
                score                REAL,
                score_score          REAL,
                composite            REAL,
                quality_score        REAL,
                name                 TEXT,
                market               TEXT,
                sector               TEXT,
                tech_score           REAL,
                pred_score           REAL,
                ret_score            REAL,
                prob_score           REAL,
                qual_score           REAL,
                safety_score         REAL,
                liquidity_score      REAL,
                final_score          REAL,
                risk_penalty         REAL,
                market_up            INTEGER,
                market_status_date   DATE,
                market_kospi_close   REAL,
                market_kospi_ma20    REAL,
                market_vol_5d        REAL,
                market_foreign_5d    REAL,
                generated_at         TEXT,
                model_version        TEXT,
                PRIMARY KEY (date, code)
            );
            """
        )
        records = df_out.to_dict(orient="records")
        conn.executemany(
            """
            INSERT OR REPLACE INTO daily_ranking
            (date, code, close, pred_return_60d, pred_return_90d, pred_mdd_60d, pred_mdd_90d,
             prob_top20_60d, prob_top20_90d, score, score_score, composite, quality_score,
             name, market, sector, tech_score, pred_score, ret_score, prob_score, qual_score,
             safety_score, liquidity_score, final_score, risk_penalty, market_up,
             market_status_date, market_kospi_close, market_kospi_ma20, market_vol_5d, market_foreign_5d,
             generated_at, model_version)
            VALUES (:date, :code, :close, :pred_return_60d, :pred_return_90d, :pred_mdd_60d, :pred_mdd_90d,
                    :prob_top20_60d, :prob_top20_90d, :score, :score_score, :composite, :quality_score,
                    :name, :market, :sector, :tech_score, :pred_score, :ret_score, :prob_score, :qual_score,
                    :safety_score, :liquidity_score, :final_score, :risk_penalty, :market_up,
                    :market_status_date, :market_kospi_close, :market_kospi_ma20, :market_vol_5d, :market_foreign_5d,
                    :generated_at, :model_version)
            """,
            records,
        )
        conn.commit()
        logging.info("Saved ranking to DB: %s (rows=%d)", DB_PATH.resolve(), len(df_out))
    except Exception:
        logging.exception("Failed to save ranking to DB")
    finally:
        try:
            if conn:
                conn.close()
        except Exception:
            pass


def main() -> None:
    setup_logging()
    ranking = build_ranking()
    save_ranking(ranking)


if __name__ == "__main__":
    main()
